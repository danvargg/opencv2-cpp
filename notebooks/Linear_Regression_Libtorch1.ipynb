{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font style=\"color:rgb(50,120,229)\">Linear Regression using Libtorch</font>\n",
    "\n",
    "In this chapter, we will show an example of using a Neural Network for predicting housing prices. The same problem can be solved using a technique called **Linear Regression**. In this unit, we will use Libtorch to create a simple neural network for linear regression.\n",
    "\n",
    "But before going into that, let's look at what Linear Regression is and the problem we want to solve.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font style=\"color:rgb(50,120,229)\">What is Linear regression?</font>\n",
    "\n",
    "<img src=\"https://www.learnopencv.com/wp-content/uploads/2018/02/cv4faces-mod10-ch2-linreg-example.png\" width=\"600\" />\n",
    "Linear regression is a linear approach to model the relationship between two variables. For example, the values on the x-axis are independent variables ( normally referred to as Samples ), and the values on the y-axis are dependent variables ( also known as Target). In the figure above, there are 5 points. We want to find a straight line that will minimize the sum of all errors ( shown by arrows in the above figure ). We want to find the slope of the line with the least error. Once we are able to model the given data points, we can predict the value on the y-axis for a new point on the x-axis.\n",
    "\n",
    "We will learn how to create a simple network with a single layer to perform linear regression. We will use the [**Boston Housing dataset**](https://www.cs.toronto.edu/~delve/data/boston/bostonDetail.html) as an example. Samples contain 13 attributes of houses at different locations around the Boston suburbs in the late 1970s. Some example attributes are the average number of rooms, crime rate, etc. You can find the complete list of attributes [**here**](https://www.cs.toronto.edu/~delve/data/boston/bostonDetail.html).\n",
    "\n",
    "The `13` attributes become our `13`-dimensional independent variable. Targets are the median values of the houses at a location (in `k$`). With the `13` features, we have to train the model, which would predict the price of the house in the validation data.\n",
    "\n",
    "A schematic diagram of the network we want to create is given below\n",
    "<img src=\"https://www.learnopencv.com/wp-content/uploads/2019/12/regression-keras-schema.png\" width=\"400\" />\n",
    "\n",
    "The purpose of training is to find the weights (`w0` to `w12`) and bias (`b`) for which the network produces the correct output by looking at the input data. We say that the network is trained when the error between the predicted output and ground truth becomes very low and does not decrease further. We can then use these weights to predict the output for any new data.\n",
    "\n",
    "The network consists just one neuron. We use the libtorch Module API to create the network graph. Then we add a Dense layer with the number of inputs equal to the number of features in the data (`13` in this case) and a single output. Then we follow the workflow as explained in the previous section, i.e. We compile the model and train it as we discussed in previous section.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font style = \"color:rgb(50,120,229)\">Import Libraries</font>Â¶\n",
    "\n",
    "Libtorch is a C++ library, which has an underlying foundation of PyTorch. For using the library, we will include \"`torch/torch.h`\" header file. This will include all functions and classes implemented in LibTorch. This is not the ideal, but we need not worry about efficiency for the time being."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```cpp\n",
    "#include <stdint.h>\n",
    "#include <torch/torch.h>\n",
    "#include <iostream>\n",
    "#include <string>\n",
    "#include <vector>\n",
    "#include \"CSVReader.h\"\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we define some constants which are used later during training:\n",
    "- `trainBatchSize`: Number of training examples used in one iteration.\n",
    "- `testBatchSize`: Number of test examples used in one iteration.\n",
    "- `epochs`: This is the number of times the algorithm sees the entire dataset during training.\n",
    "- `logInterval`: Number of iteration after which the training stats should print.\n",
    "- `datasetPath`: Path of the data file (BostonHousing.csv)\n",
    "- `device`: Specify whether we have to use CPU or GPU. By default, it takes CPU. As we saw in the sample code, we can check whether GPU is available. Accordingly, we can update torch device."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```cpp\n",
    "struct Options {\n",
    "  size_t trainBatchSize = 4;\n",
    "  size_t testBatchSize = 100;\n",
    "  size_t epochs = 1000;\n",
    "  size_t logInterval = 20;\n",
    "  // path must end in delimiter\n",
    "  std::string datasetPath = \"./BostonHousing.csv\";\n",
    "  // For CPU use torch::kCPU and for GPU use torch::kCUDA\n",
    "  torch::DeviceType device = torch::kCPU;\n",
    "};\n",
    "\n",
    "static Options options;\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font style = \"color:rgb(50,120,229)\">Data Pre-processing</font>\n",
    "\n",
    "Let us first define the pre-processing done on the data. \n",
    "\n",
    "We perform normalization on the features. It is also called **Scaling** and is required when different features have different ranges. \n",
    "\n",
    "For example, consider a data set containing two features - \n",
    "- the number of bedrooms, `x1`, and \n",
    "- the cost of the flat (in `$`), `x2`. \n",
    "\n",
    "`x1` may have range `1-4`, whereas `x2` may have range `100,000-1,000,000`. If we do not normalize the data, the cost feature will have more influence on the learned model, which eventually means that the cost feature is a much more important feature than the number of bedrooms, which may or may not be True. So, we will normalize each feature between `0-1` and let the model decide the prominent feature during training.\n",
    "\n",
    "We will carry out min-max normalization as the data pre-processing step so that all the feature values lie between `0` and `1`.\n",
    "\n",
    "We will read values from a csv file as a string and convert it to float. For each feature, we will find the maximum and the minimum value. Finally, to normalize the feature, we will subtract the minimum value and then divide by the maximum range of value."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```cpp\n",
    "std::vector<std::vector<float>> normalize_feature(std::vector<std::vector<std::string> > feat, int rows, int cols) {\n",
    "  std::vector<float> input(cols, 1);\n",
    "  std::vector<std::vector<float>> data(rows, input);\n",
    "\n",
    "  for (int i = 0; i < cols; i++) {   // each column has one feature\n",
    "      // initialize the maximum element with 0 \n",
    "      // std::stof is used to convert string to float\n",
    "      float maxm = std::stof(feat[1][i]);\n",
    "      float minm = std::stof(feat[1][i]);\n",
    "\n",
    "      // Run the inner loop over rows (all values of the feature) for given column (feature) \n",
    "      for (int j = 1; j < rows; j++) { \n",
    "          // check if any element is greater  \n",
    "          // than the maximum element \n",
    "          // of the column and replace it \n",
    "          if (std::stof(feat[j][i]) > maxm) \n",
    "              maxm = std::stof(feat[j][i]);\n",
    "\n",
    "          if (std::stof(feat[j][i]) < minm)\n",
    "              minm = std::stof(feat[j][i]); \n",
    "      } \n",
    "      \n",
    "      // From above loop, we have min and max value of the feature\n",
    "      // Will use min and max value to normalize values of the feature\n",
    "      for (int j = 0; j < rows-1; j++) { \n",
    "        // Normalize the feature values to lie between 0 and 1\n",
    "        data[j][i] = (std::stof(feat[j+1][i]) - minm)/(maxm - minm); \n",
    "      }\n",
    "  }\n",
    "\n",
    "  return data;\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font style = \"color:rgb(50,120,229)\">Create Dataset</font>\n",
    "We read the data from the csv file - `BostonHousing.csv`. Then, We split the data into two parts - training and validation (will not be used for training). We will split the data into training and validation set with an `80:20` ratio. \n",
    "\n",
    "We normalize the features using the function defined above and then shuffle the data before passing it to the training process.\n",
    "\n",
    "The model will train on the training dataset, and the model evaluation will be on the validation dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```cpp\n",
    "// Define Data to accomodate pairs of (input_features, output)\n",
    "using Data = std::vector<std::pair<std::vector<float>, float>>;\n",
    "\n",
    "std::pair<Data, Data> readInfo() {\n",
    "  Data train, test;\n",
    "\n",
    "  // Reads data from CSV file.\n",
    "  // CSVReader class is defined in CSVReader.h header file\n",
    "  CSVReader reader(options.datasetPath);\n",
    "  std::vector<std::vector<std::string> > dataList = reader.getData();\n",
    "\n",
    "\n",
    "  int N = dataList.size();      // Total number of data points\n",
    "  // As last column is the output, feature size will be number of column minus one.\n",
    "  int fSize = dataList[0].size() - 1;\n",
    "  std::cout << \"Total number of features: \" << fSize << std::endl;\n",
    "  std::cout << \"Total number of data points: \" << N << std::endl;\n",
    "  int limit = 0.8*N;    // 80 percent data for training and rest 20 percent for validation\n",
    "  std::vector<float> input(fSize, 1);\n",
    "  std::vector<std::vector<float>> data(N, input);\n",
    "\n",
    "  // Normalize data\n",
    "  data = normalize_feature(dataList, N, fSize);\n",
    "\n",
    "\n",
    "  for (int i=1; i < N; i++) {\n",
    "    for (int j= 0; j < fSize; j++) {\n",
    "        input[j] = data[i-1][j];\n",
    "    }\n",
    "\n",
    "    float output = std::stof(dataList[i][fSize]);\n",
    "\n",
    "    // Split data data into train and test set\n",
    "    if (i <= limit) {\n",
    "      train.push_back({input, output});\n",
    "    } else {\n",
    "      test.push_back({input, output});\n",
    "    }\n",
    "  }\n",
    "\n",
    "  std::cout << \"Total number of training data: \" << train.size() << std::endl;\n",
    "  std::cout << \"Total number of test data: \" << test.size() << std::endl;\n",
    "\n",
    "  // Shuffle training data\n",
    "  std::random_shuffle(train.begin(), train.end());\n",
    "\n",
    "  return std::make_pair(train, test);\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font style = \"color:rgb(50,120,229)\">Load Dataset using Torch</font>\n",
    "Pytorch has a few in-built datasets which can be directly loaded. For custom datasets, we can write custom functions to read our data.\n",
    "\n",
    "Here, we have written a custom data loader to read CSV file - `CSVReader.h`. It reads a line from the file and stores it as a string. The whole csv file can then be read as a vector of strings."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```cpp\n",
    "/*\n",
    " * A class to read data from a csv file.\n",
    " */\n",
    "class CSVReader\n",
    "{\n",
    "  std::string fileName;\n",
    "  std::string delimeter;\n",
    " \n",
    "public:\n",
    "  CSVReader(std::string filename, std::string delm = \",\") :\n",
    "      fileName(filename), delimeter(delm)\n",
    "  { }\n",
    " \n",
    "  // Function to fetch data from a CSV File\n",
    "  std::vector<std::vector<std::string> > getData();\n",
    "};\n",
    " \n",
    "/*\n",
    "* Parses through csv file line by line and returns the data\n",
    "* in vector of vector of strings.\n",
    "*/\n",
    "std::vector<std::vector<std::string> > CSVReader::getData()\n",
    "{\n",
    "  std::ifstream file(fileName);\n",
    " \n",
    "  std::vector<std::vector<std::string> > dataList;\n",
    " \n",
    "  std::string line = \"\";\n",
    "  // Iterate through each line and split the content using delimeter\n",
    "  while (getline(file, line))\n",
    "  {\n",
    "    std::vector<std::string> vec;\n",
    "    boost::algorithm::split(vec, line, boost::is_any_of(delimeter));\n",
    "    dataList.push_back(vec);\n",
    "  }\n",
    "  // Close the File\n",
    "  file.close();\n",
    " \n",
    "  return dataList;\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font style = \"color:rgb(50,120,229)\">Neural Network</font>\n",
    "Let's create the model that we will use to solve the linear regression problem.\n",
    "\n",
    "Neural networks based on the C++ frontend are composed of reusable building blocks called modules.\n",
    "\n",
    "[**`torch::nn::Module`**](https://pytorch.org/docs/stable/nn.html) is the base module class from which all other modules are derived. Each module can contain subobjects like parameters, buffers, and submodules. \n",
    "\n",
    "These subobjects are explicitly registered. For this linear regression problem, we will need to register a dense layer (`torch::nn::Linear`) \n",
    "\n",
    "We also need to implement the forward pass for the network. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```cpp\n",
    "// Linear Regression Model\n",
    "struct Net : torch::nn::Module {\n",
    "  /*\n",
    "  Network for Linear Regression is just a single neuron (i.e. one Dense Layer) \n",
    "  Usage: auto net = std::make_shared<Net>(num_features, num_outputs) \n",
    "  */\n",
    "  Net(int num_features, int num_outputs) {\n",
    "    neuron = register_module(\"neuron\", torch::nn::Linear(num_features, num_outputs));\n",
    "    }\n",
    "\n",
    "  torch::Tensor forward(torch::Tensor x) {\n",
    "    /*Convert row tensor to column tensor*/\n",
    "    x = x.reshape({x.size(0), -1});\n",
    "    /*Pass the input tensor through linear function*/\n",
    "    x = neuron->forward(x);\n",
    "    return x;\n",
    "  }\n",
    "\n",
    "  /*Initilaize the constructor with null pointer. More details given in the reference*/\n",
    "  torch::nn::Linear neuron{ nullptr };\n",
    "};\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font style = \"color:rgb(50,120,229)\">Train the Model</font>\n",
    "Training data is passed in a set of batches, and mean square error loss function is used to calculate the loss. Then the loss function is passed through a stochastic gradient descent optimizer with learning rate 0.001."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```cpp\n",
    "template <typename DataLoader>\n",
    "void train(std::shared_ptr<Net> network, DataLoader& loader, torch::optim::Optimizer& optimizer, \n",
    "           size_t epoch, size_t data_size) {\n",
    "  size_t index = 0;\n",
    "  /*Set network in the training mode*/\n",
    "  network->train();\n",
    "  float Loss = 0;\n",
    "\n",
    "  for (auto& batch : loader) {\n",
    "    auto data = batch.data.to(options.device);\n",
    "    auto targets = batch.target.to(options.device).view({-1});\n",
    "    // Execute the model on the input data\n",
    "    auto output = network->forward(data);\n",
    "\n",
    "    //Using mean square error loss function to compute loss\n",
    "    auto loss = torch::mse_loss(output, targets);\n",
    "\n",
    "    // Reset gradients\n",
    "    optimizer.zero_grad();\n",
    "    // Compute gradients\n",
    "    loss.backward();\n",
    "    //Update the parameters\n",
    "    optimizer.step();\n",
    "\n",
    "    Loss += loss.template item<float>();\n",
    "\n",
    "    if (index++ % options.logInterval == 0) {\n",
    "      auto end = std::min(data_size, (index + 1) * options.trainBatchSize);\n",
    "\n",
    "      std::cout << \"Train Epoch: \" << epoch << \" \" << end << \"/\" << data_size\n",
    "                << \"\\t\\tLoss: \" << Loss / end << std::endl;\n",
    "    }\n",
    "  }\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font style = \"color:rgb(50,120,229)\">Test the Model</font>\n",
    "Similar to the above, the test data is passed through the trained network to evaluate the model.  Loss is calculated on test data. Sample output at index 5 is printed along with its "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```cpp\n",
    "template <typename DataLoader>\n",
    "void test(std::shared_ptr<Net> network, DataLoader& loader, size_t data_size) {\n",
    "  network->eval();\n",
    "\n",
    "  for (const auto& batch : loader) {\n",
    "    auto data = batch.data.to(options.device);\n",
    "    auto targets = batch.target.to(options.device).view({-1});\n",
    "\n",
    "    auto output = network->forward(data);\n",
    "    std::cout << \"Predicted:\"<< output[0].template item<float>() << \"\\t\" << \"Groundtruth: \"\n",
    "            << targets[1].template item<float>() << std::endl;\n",
    "    std::cout << \"Predicted:\"<< output[1].template item<float>() << \"\\t\" << \"Groundtruth: \"\n",
    "            << targets[1].template item<float>() << std::endl;\n",
    "    std::cout << \"Predicted:\"<< output[2].template item<float>() << \"\\t\" << \"Groundtruth: \"\n",
    "            << targets[2].template item<float>() << std::endl;\n",
    "    std::cout << \"Predicted:\"<< output[3].template item<float>() << \"\\t\" << \"Groundtruth: \"\n",
    "            << targets[3].template item<float>() << std::endl;\n",
    "    std::cout << \"Predicted:\"<< output[4].template item<float>() << \"\\t\" << \"Groundtruth: \"\n",
    "            << targets[4].template item<float>() << std::endl;\n",
    "\n",
    "    auto loss = torch::mse_loss(output, targets);\n",
    "\n",
    "    break;\n",
    "  }\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font style = \"color:rgb(50,120,229)\">Main Function</font>\n",
    "Main function contains following steps:\n",
    "- **Data Processing**\n",
    "    - Read the data from csv file, normalize the data and split into train and test data.\n",
    "- **Data Loader**\n",
    "    - This provides options for batch size, number of workers to be used to speed up the data loading.\n",
    "- **Model Initialization**\n",
    "    - Define network parameters\n",
    "- **Training**\n",
    "    - Call the train function epoch number of times and observe the loss values.\n",
    "- **Testing**\n",
    "    - Call the test function in each epoch and observe the loss values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```cpp\n",
    "int main() {\n",
    "  /*Sets manual seed from libtorch random number generators*/\n",
    "  torch::manual_seed(1);\n",
    "\n",
    "  /*Use CUDA for computation if available*/\n",
    "  if (torch::cuda::is_available())\n",
    "    options.device = torch::kCUDA;\n",
    "  std::cout << \"Running on: \"\n",
    "            << (options.device == torch::kCUDA ? \"CUDA\" : \"CPU\") << std::endl;\n",
    "\n",
    "  /*Read data and split data into train and test sets*/\n",
    "  auto data = readInfo();\n",
    "\n",
    "  /*Uses Custom Dataset Class to load train data. Apply stack collation which takes \n",
    "  batch of tensors and stacks them into single tensor along the first dimension*/\n",
    "  auto train_set =\n",
    "      CustomDataset(data.first).map(torch::data::transforms::Stack<>());\n",
    "  auto train_size = train_set.size().value();\n",
    "\n",
    "  /*Data Loader provides options to speed up the data loading like batch size, number of workers*/\n",
    "  auto train_loader =\n",
    "      torch::data::make_data_loader(\n",
    "          std::move(train_set), options.trainBatchSize);\n",
    "\n",
    "  std::cout << train_size << std::endl;\n",
    "  /*Uses Custom Dataset Class to load test data. Apply stack collation which takes \n",
    "  batch of tensors and stacks them into single tensor along the first dimension*/\n",
    "  auto test_set =\n",
    "      CustomDataset(data.second).map(torch::data::transforms::Stack<>());\n",
    "  auto test_size = test_set.size().value();\n",
    "\n",
    "  /*Test data loader similar to train data loader*/\n",
    "  auto test_loader =\n",
    "      torch::data::make_data_loader(\n",
    "          std::move(test_set), options.testBatchSize);\n",
    "  /*Create Linear  Regression Network*/\n",
    "  auto net = std::make_shared<Net>(13, 1);\n",
    "\n",
    "  /*Moving model parameters to correct device*/\n",
    "  net->to(options.device);\n",
    "\n",
    "  /*Using stochastic gradient descent optimizer with learning rate 0.000001*/\n",
    "  torch::optim::SGD optimizer(\n",
    "       net->parameters(), torch::optim::SGDOptions(0.000001));\n",
    "\n",
    "  std::cout << \"Training...\" << std::endl;\n",
    "  for (size_t i = 0; i < options.epochs; ++i) {\n",
    "    /*Run the training for all iterations*/\n",
    "    train(net, *train_loader, optimizer, i + 1, train_size);\n",
    "    std::cout << std::endl;\n",
    "\n",
    "    if (i == options.epochs - 1) {\n",
    "      std::cout << \"Testing...\" << std::endl;\n",
    "      test(net, *test_loader, test_size);\n",
    "    }\n",
    "  }\n",
    "\n",
    "\n",
    "  return 0;\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font style=\"color:blue\">Steps to Compile and Run the Code on Google Colab</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font style=\"color:green\">Download LibTorch</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget https://download.pytorch.org/libtorch/cu101/libtorch-shared-with-deps-1.3.1.zip -O libtorch.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!unzip libtorch.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm -r libtorch.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font style=\"color:green\">Download Code</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget \"https://www.dropbox.com/s/rx6zsq5wmc18p2p/linear-regression.zip?dl=1\" -O linear-regression.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!unzip linear-regression.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir(\"linear-regression\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font style=\"color:green\">Compile</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cmake -DCMAKE_PREFIX_PATH=$PWD/../libtorch ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!make"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font style=\"color:green\">Run </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!./lregression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font style = \"color:rgb(50,120,229)\">References</font>\n",
    "\n",
    "https://pytorch.org/tutorials/advanced/cpp_frontend.html\n",
    "\n",
    "https://github.com/pytorch/examples/blob/master/cpp/custom-dataset/custom-dataset.cpp"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
